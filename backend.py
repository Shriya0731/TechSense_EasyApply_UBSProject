# -*- coding: utf-8 -*-
"""backend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lqes_0uM1KKKD1GySJtZ9D8Ub-trOExh
"""


import warnings

warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.gridspec import GridSpec
import re
import nltk.corpus
import os
import pandas as pd
from fuzzywuzzy import fuzz
import DBConnection as db
nltk.download('stopwords')
from nltk.corpus import stopwords
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
from sklearn.multiclass import OneVsRestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

def cleanprofilesdataset():
    #resumeDataSet = pd.read_csv('skills.csv', encoding='utf-8')
    resumeDataSet = pd.DataFrame(db.getallprofiles())

    resumeDataSet['cleaned_resume'] = resumeDataSet[2].apply(lambda x: cleanResume(str(x)))
    var_mod =[2]
    le = LabelEncoder()
    for i in var_mod:
        resumeDataSet[i] = le.fit_transform(resumeDataSet[i])
    requiredText = resumeDataSet['cleaned_resume'].values
    requiredTarget = resumeDataSet[2].values
    word_vectorizer = TfidfVectorizer(
        sublinear_tf=True,
        stop_words='english',
        max_features=1500)
    word_vectorizer.fit(requiredText)
    WordFeatures = word_vectorizer.transform(requiredText)
    stop_words = stopwords.words('english')
    resumeDataSet['cleaned_resume'] = resumeDataSet['cleaned_resume'].apply(
        lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
    resume1st = resumeDataSet['cleaned_resume'][0].lower()
    print("Checking",resumeDataSet)
    return  resumeDataSet


def cleanResume(resumeText):
    resumeText = re.sub('httpS+s*', ' ', resumeText)  # remove URLs
    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc
    resumeText = re.sub('#S+', '', resumeText)  # remove hashtags
    resumeText = re.sub('@S+', '  ', resumeText)  # remove mentions
    resumeText = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[]^_`{|}~"""), ' ',
                        resumeText)  # remove punctuations

    # resumeText = re.sub('s+', ' ', resumeText)  # remove extra whitespace
    return resumeText




def cleanjd(job_desc):

    cleaned_jd = cleanResume(job_desc)

    le = LabelEncoder()

    cleaned_jd = le.fit_transform(cleaned_jd)
    requiredText1 = cleaned_jd.values
    word_vectorizer = TfidfVectorizer(
        sublinear_tf=True,
        stop_words='english',
        max_features=1500)
    word_vectorizer.fit(requiredText1)
    WordFeatures = word_vectorizer.transform(requiredText1)

    stop_words = stopwords.words('english')
    cleaned_jd = cleaned_jd.apply(
        lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
    print(cleaned_jd)
    cleaned_jd =cleaned_jd.lower()
    stop_words = stopwords.words('english')
    cleaned_jd= cleaned_jd.apply(
        lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
    #resume1st = resumeDataSet['cleaned_resume'][0].lower()
    return cleaned_jd


def findsimilarityscore(job_desc,skillsdataset):

    count=0
    for i in skillsdataset['cleaned_resume']:

        Resume = i.lower()
        # desc =jd_1st
        tuples_list = [max([(fuzz.token_set_ratio(job_desc, Resume), Resume)])]
        # Unpack list of tuples into two  for j in resume1st]) for i in jd_1st
        similarity_score, fuzzy_match = map(list, zip(*tuples_list))

        # Create pandas DataFrame
        df = pd.DataFrame({"Name":skillsdataset[0][count], "similarity score": similarity_score})
        db.addsimilarityscore(skillsdataset[0][count],similarity_score[0])
        count = count+1
        print(df)


# skillsdata = cleanprofilesdataset()
# jd_desc = db.getjobdesc('Full Stack Developer')
# print(jd_desc)
# findsimilarityscore(jd_desc,skillsdata)